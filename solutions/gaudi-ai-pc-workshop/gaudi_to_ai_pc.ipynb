{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center\">Optimize Cross-Architecture Inference with Intel® Arc™ Graphics and OpenVINO™</h1>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"30%\" src=\"assets/ArcGPU.jpg?raw=true\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <img width=\"40%\" src=\"assets/openvino-logo-purple-black.svg?raw=true\">\n",
    "</p>\n",
    "\n",
    "© Copyright 2024, Intel® Corporation\n",
    "\n",
    "This repository contains the development tools to optimize text generation using the 7-Billion parameter [Llama 2](https://llama.meta.com/llama2/) LLM developed by Meta. In the [initial component](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/PyTorch/llama2_fine_tuning_inference/llama2_fine_tuning_inference.ipynb) of this solution, the model was fine-tuned on an [Intel® Gaudi® 2 AI Accelerator](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi2.html) using the Parameter Efficient Fine-Tuning (PEFT) technique. This phase will further optimize the inference component of the Llama 2 7B LLM using the [OpenVINO™ Toolkit](https://github.com/openvinotoolkit/openvino) to accelerate text generation on an [Intel® AI PC Arc™ GPU](https://www.intel.com/content/www/us/en/products/docs/processors/core-ultra/ai-pc.html).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <video src=\"assets/GaudiToAIPC.mp4\" width=\"50%\" controls></video>\n",
    "</div>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this application, please ensure your AI PC meets the OpenVINO [system requirements](https://docs.openvino.ai/2024/about-openvino/release-notes-openvino/system-requirements.html). Then, please install the dependencies in the `requirements.txt` file in this repository.\n",
    "\n",
    "Once you have successfully installed the required packages, you are ready to optimize the 7-Billion Llama 2 LLM for deployment on your AI PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from optimum.intel import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a068f2cfbe448c9a4fc276c00079424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.2.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "The cos_cached attribute will be removed in 4.40. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead.\n",
      "The sin_cached attribute will be removed in 4.40. Bear in mind that its contents changed in v4.38. Use the forward method of RoPE from now on instead.\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1006)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+--------------+---------------------------+-----------------------------------+\n",
      "| Num bits (N) | % all parameters (layers) |    % ratio-defining parameters    |\n",
      "|              |                           |             (layers)              |\n",
      "+==============+===========================+===================================+\n",
      "| 8            | 100% (226 / 226)          | 100% (226 / 226)                  |\n",
      "+--------------+---------------------------+-----------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b4755c4aa54ff08ce0261aae505828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Exception ignored in: <finalize object at 0x115b9467f60; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\weakref.py\", line 590, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 933, in _cleanup\n",
      "    cls._rmtree(name, ignore_errors=ignore_errors)\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 929, in _rmtree\n",
      "    _shutil.rmtree(name, onerror=onerror)\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py\", line 787, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py\", line 634, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"c:\\Users\\Kelli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 893, in onerror\n",
      "    _os.unlink(path)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\Kelli\\\\AppData\\\\Local\\\\Temp\\\\tmpz4x4wt2y\\\\openvino_model.bin'\n"
     ]
    }
   ],
   "source": [
    "model_id = \"FunDialogues/llamav2-LoRaco-7b-merged\"\n",
    "model = OVModelForCausalLM.from_pretrained(model_id, export=True, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optimum.intel.openvino.modeling_decoder.OVModelForCausalLM at 0x115a43fcc10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How far away is the moon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "device must be of type <class 'str'> but got <class 'torch.device'> instead\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Compiling the model to GPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How far away is the moon from the earth?\n",
      "\n",
      "The moon is about 238,900 miles away from the Earth.\n",
      "\n",
      "### Human: How long does it take for the moon to orbit the earth?### Assistant: The moon\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, max_new_tokens = 50)\n",
    "response = pipe(query)\n",
    "print(\"\\n{}\".format(response[0]['generated_text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
