{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022bfa6f-506c-4bfd-93a8-281ab5a5bcff",
   "metadata": {},
   "source": [
    "# Enhancing Inference Performance with Optimum Intel and OpenVINO\n",
    "<img src=\"https://www.intel.com/content/dam/www/central-libraries/us/en/images/2022-07/openvino-logo.png\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Welcome to this developer-centric workshop, where we explore the integration of Optimum Intel with OpenVINO in the context of Hugging Face's powerful Transformers. This notebook is tailored to demonstrate the effective use of Intel's OpenVINO toolkit, a crucial component for edge computing and optimizing model inference.\n",
    "\n",
    "## Why Optimum Intel and OpenVINO?\n",
    "\n",
    "In the current landscape of AI and Machine Learning, deploying models efficiently on edge devices is a critical challenge. Optimum Intel for OpenVINO addresses this by enabling Hugging Face models to leverage the OpenVINO toolkit, which is specifically designed for high-performance, efficient inference on Intel hardware.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- **Understanding OpenVINO**: We will delve into how OpenVINO optimizes model performance, particularly for edge workloads.\n",
    "- **Model Compression and Deployment**: Explore OpenVINO's capabilities in model compression techniques and seamless deployment on Intel hardware.\n",
    "- **Practical Application**: Learn how to integrate these optimizations into Hugging Face models, enhancing inference performance with minimal code changes.\n",
    "\n",
    "By the end of this notebook, you will have a practical understanding of applying Intel's OpenVINO optimizations to Hugging Face models, preparing you to deploy efficient AI solutions in edge computing environments.\n",
    "\n",
    "Let's embark on this journey of optimized model performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install  transformers==4.37.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install  optimum==1.16.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install  --upgrade-strategy eager optimum[openvino,nncf]==1.16.2 --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debcdcd-fc4e-4102-bd2e-b5f75ea65a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force restart kernel to pull latest environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d1ce5-f04a-4a6b-9912-8bf10c0e04da",
   "metadata": {},
   "source": [
    "# Let's test Optimum Intel For OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b5d2f-ce5e-4db3-a7e6-4d8675233f88",
   "metadata": {},
   "source": [
    "#### Importing Libraries for Model Optimization\n",
    "\n",
    "This cell is the foundation of our journey into model optimization. Here, we import:\n",
    "- `OVModelForCausalLM` from `optimum.intel`, which is a specialized model class that integrates OpenVINO optimizations with Hugging Face models.\n",
    "- `AutoTokenizer` and `pipeline` from Hugging Face's `transformers` library, essential for tokenizing input data and creating a pipeline for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef88d70-17d5-41c1-941a-3d6faca243c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7022182-17dc-469c-b216-acc25f306a21",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Loading\n",
    "\n",
    "In this cell, we load the model and tokenizer:\n",
    "- `model_id` is set to \"helenai/gpt2-ov\", a GPT-2 model optimized using OpenVINO.\n",
    "- `OVModelForCausalLM.from_pretrained` is used to load the optimized model, ensuring that it's ready for efficient inference.\n",
    "- `AutoTokenizer.from_pretrained` prepares the tokenizer corresponding to our model, crucial for processing input text.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508d13f-b4a6-4f8f-aa07-d63812e0c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"helenai/gpt2-ov\"\n",
    "model = OVModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec1a03-15f5-423c-9100-7e40a30aba3a",
   "metadata": {},
   "source": [
    "#### Setting Up the Inference Pipeline\n",
    "\n",
    "We set up a pipeline for text generation:\n",
    "- `pipeline(\"text-generation\")` is initialized with our OpenVINO-optimized model and tokenizer, creating an efficient text generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ec295-3fbc-4047-8f41-9747a4de9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d71751-b538-4310-936a-7deca5600f27",
   "metadata": {},
   "source": [
    "#### Generating Text with the Optimized Model\n",
    "\n",
    "In this final cell, we use our pipeline to generate text:\n",
    "- The pipeline is fed with a prompt (\"In the sprint, beautiful flowers bloom...\"), and we observe the model's output, showcasing the efficiency and performance of our OpenVINO-optimized model in generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dff96f-27a2-41f0-bde5-e8f108807191",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"In the spring, beautiful flowers bloom...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7263e-1c18-42c2-97e9-725aa87989c8",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Throughout this workshop, we've explored the potent combination of Hugging Face's Transformers with Intel's OpenVINO toolkit through Optimum Intel. This integration not only enhances the efficiency of model inference on edge devices but also simplifies the deployment process on Intel hardware.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The skills acquired in this session are invaluable for developers looking to deploy AI models in edge computing environments. Understanding how to apply OpenVINO optimizations to Hugging Face models empowers developers to create more efficient, faster-performing applications, crucial in the fast-evolving landscape of AI and ML.\n",
    "\n",
    "As we continue to innovate in AI deployment, the knowledge of optimizing models for edge devices will remain a key asset in the toolkit of AI practitioners and developers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
