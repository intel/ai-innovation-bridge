{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0541c12e-12f7-4d23-bc14-0590672c6979",
   "metadata": {},
   "source": [
    "# Leveraging Intel Optimizations with Hugging Face for Enhanced Model Performance\n",
    "\n",
    "Welcome to this developer-centric workshop where we explore the synergies between Hugging Face and Intel Extension for PyTorch (IPEX). This notebook serves as an introduction to utilizing IPEX for fine-tuning a pre-trained model, specifically focusing on the `distilbert-base-uncased` model for multi-class emotion classification in text.\n",
    "\n",
    "## Why This is Important\n",
    "\n",
    "Understanding how to leverage Intel optimizations is crucial for developers looking to maximize computational efficiency and performance. By integrating IPEX with Hugging Face's API, we can significantly enhance training speeds, especially when utilizing mixed precision training with FP32 and BF16. This notebook will demonstrate these capabilities practically, offering insights into:\n",
    "\n",
    "- How to enable IPEX within Hugging Face's `TrainingArguments` and training functions.\n",
    "- Comparing training speeds and efficiencies between IPEX-enabled and standard training processes.\n",
    "- Performing inference to assess the model's accuracy in classifying emotions.\n",
    "\n",
    "## Acquiring the Learnings\n",
    "\n",
    "Through step-by-step instructions, hands-on examples, and comparative analyses, this workshop will equip you with the skills to effectively integrate Intel's optimizations into your NLP projects using Hugging Face. Let's dive into the world where cutting-edge language processing meets optimized computational performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501a047-ca9b-41d9-a1ca-ec3e1613c573",
   "metadata": {},
   "source": [
    "#### Environment Setup and Dependencies Installation\n",
    "\n",
    "This cell prepares our working environment. It sources Intel oneAPI for optimal performance on Intel hardware (optional based on your setup) and installs specific versions of essential libraries: `transformers`, `torch`, and `intel_extension_for_pytorch`. These installations ensure we have the necessary tools to leverage Intel's optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers==4.44.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install torch==2.2.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install intel_extension_for_pytorch==2.2.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install datasets==2.16.1 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install accelerate==0.32.0 --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb853ac-e1a6-41dc-9b68-da66b010e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force restart kernel to pull latest environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d9300-aa72-4cb2-bfd9-ee5c2ca391af",
   "metadata": {},
   "source": [
    "#### Loading Libraries and Packages\n",
    "\n",
    "In this cell, we import the core libraries that will be used throughout the notebook. This setup is crucial as it prepares our Python environment with all the necessary tools for our tasks.\n",
    "\n",
    "- `from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments`: We import key components from the Hugging Face Transformers library. `AutoModelForSequenceClassification` and `AutoTokenizer` are used for loading the model and tokenizer, respectively. `Trainer` and `TrainingArguments` are essential for setting up and running our model training.\n",
    "- `from datasets import load_dataset`: This import from the `datasets` library allows us to easily load and preprocess datasets available in Hugging Face's datasets hub.\n",
    "- `import numpy as np`: Numpy is a fundamental package for scientific computing in Python. It provides support for arrays, mathematical operations, and various utility functions.\n",
    "- `from sklearn.metrics import accuracy_score`: We import the `accuracy_score` function from Scikit-Learn to calculate the accuracy of our model predictions during evaluation. This metric will help us quantify the performance of our fine-tuned model.\n",
    "\n",
    "Overall, this cell lays the foundation for our machine learning tasks by equipping us with the necessary libraries and modules.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4acc2-5808-449f-a55b-bd3484ad3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22e9f-4e4e-4672-bdba-b264f5893d41",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "Here, we load the `emotion` dataset from Hugging Face's datasets library. This dataset will be used for training and evaluating our DistilBERT model, providing a practical context for emotion classification in text.k..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89638882-e384-46a7-93c4-de9b60d33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc4f80-7806-4bc8-b251-dd3d8e1d5189",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Initialization\n",
    "\n",
    "In this cell, we initialize the `distilbert-base-uncased` model and its corresponding tokenizer for sequence classification. This setup is the first step in preparing our model for fine-tuning on the emotion classification task..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dcc84-5752-489f-9e0d-adcd1b8201a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad31f1-2669-4018-9629-ff6ba02af7d3",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Data preprocessing is essential for model training. We define and apply a preprocessing function that tokenizes our text data, making it compatible with the DistilBERT model's input requirements.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b861c-54c8-4a43-95dc-94975ac1ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf58cd-55a0-4bec-88e5-930beecf5e78",
   "metadata": {},
   "source": [
    "#### Training with IPEX\n",
    "\n",
    "This cell is where the integration of Intel Extension for PyTorch (IPEX) comes into play. We define training arguments, including enabling BF16 and IPEX, and set up our Hugging Face trainer. The model is then trained on the emotion dataset, utilizing the enhanced capabilities provided by IPEX..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0798d9-7d90-41c0-ac7b-4d1b0370873d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    bf16=True, #False to train at FP32\n",
    "    use_ipex=True, #False to train w/o IPEX\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1046c73-336d-4edb-9210-71a50571583e",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "Post-training, we evaluate the model's performance on the validation dataset. This evaluation will give us insights into the effectiveness of our training and the accuracy of the model in emotion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7678f5-385d-44fc-857b-c501712ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7c017-dcdf-4d79-8441-0eb3eb9ae50f",
   "metadata": {},
   "source": [
    "#### Inference and Testing\n",
    "\n",
    "Finally, we test the fine-tuned model's inference capabilities on new sentences. This step involves preprocessing the test sentences, performing predictions, and mapping these predictions to human-readable labels. It allows us to visually inspect the model's ability to classify emotions in various text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300ea670-26d0-439c-8b1f-e2156ba8f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sentences\n",
    "test_sentences = [\n",
    "    \"I am feeling incredibly happy and joyful today!\",\n",
    "    \"I am so sad and down.\",\n",
    "    \"I have mixed feelings about this.\",\n",
    "    \"This is absolutely terrifying!\",\n",
    "]\n",
    "\n",
    "# Preprocess the test sentences\n",
    "encoded_input = tokenizer(test_sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Predict using the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    predictions = model(**encoded_input)\n",
    "\n",
    "# Convert predictions to human-readable labels\n",
    "predicted_labels = np.argmax(predictions.logits.numpy(), axis=1)\n",
    "\n",
    "# Mapping for the 'emotion' dataset labels\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "\n",
    "# Print predictions\n",
    "for sentence, label_id in zip(test_sentences, predicted_labels):\n",
    "    print(f\"Sentence: '{sentence}' - Emotion Prediction: {label_map[label_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46160e25-50d6-48b6-8353-851320c95794",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Throughout this workshop, we have explored the integration of Intel optimizations with Hugging Face's powerful Transformers library. By fine-tuning the DistilBERT model with the support of Intel Extension for PyTorch, we observed enhanced training speeds and efficient utilization of computational resources, especially notable in mixed precision training scenarios.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The exercise showcased not only the technical prowess of combining Hugging Face with Intel optimizations but also highlighted the practical benefits such as reduced training times and resource efficiency. This understanding is pivotal for developers working on NLP tasks, seeking to optimize model performance on Intel hardware. As AI and NLP continue to evolve, harnessing these optimizations will be key in developing more efficient and powerful AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
