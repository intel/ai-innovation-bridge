{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0541c12e-12f7-4d23-bc14-0590672c6979",
   "metadata": {},
   "source": [
    "# Leveraging Intel Optimizations with Hugging Face for Enhanced Model Performance\n",
    "\n",
    "<img src=\"https://www.developer-tech.com/wp-content/uploads/sites/3/2023/08/intel-pytorch-foundation-ai-development-artificial-intelligence-coding-programming-machine-learning.jpg\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "Welcome to this developer-centric workshop where we explore the synergies between Hugging Face and Intel Extension for PyTorch (IPEX). This notebook serves as an introduction to utilizing IPEX for fine-tuning a pre-trained model, specifically focusing on the `distilbert-base-uncased` model for multi-class emotion classification in text.\n",
    "\n",
    "## Why This is Important\n",
    "\n",
    "Understanding how to leverage Intel optimizations is crucial for developers looking to maximize computational efficiency and performance. By integrating IPEX with Hugging Face's API, we can significantly enhance training speeds, especially when utilizing mixed precision training with FP32 and BF16. This notebook will demonstrate these capabilities practically, offering insights into:\n",
    "\n",
    "- How to enable IPEX within Hugging Face's `TrainingArguments` and training functions.\n",
    "- Comparing training speeds and efficiencies between IPEX-enabled and standard training processes.\n",
    "- Performing inference to assess the model's accuracy in classifying emotions.\n",
    "\n",
    "## Acquiring the Learnings\n",
    "\n",
    "Through step-by-step instructions, hands-on examples, and comparative analyses, this workshop will equip you with the skills to effectively integrate Intel's optimizations into your NLP projects using Hugging Face. Let's dive into the world where cutting-edge language processing meets optimized computational performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501a047-ca9b-41d9-a1ca-ec3e1613c573",
   "metadata": {},
   "source": [
    "#### Environment Setup and Dependencies Installation\n",
    "\n",
    "This cell prepares our working environment. It sources Intel oneAPI for optimal performance on Intel hardware (optional based on your setup) and installs specific versions of essential libraries: `transformers`, `torch`, and `intel_extension_for_pytorch`. These installations ensure we have the necessary tools to leverage Intel's optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ":: WARNING: setvars.sh has already been run. Skipping re-execution.\n",
      "   To force a re-execution of setvars.sh, use the '--force' option.\n",
      "   Using '--force' can result in excessive use of your environment variables.\n",
      "  \n",
      "usage: source setvars.sh [--force] [--config=file] [--help] [...]\n",
      "  --force        Force setvars.sh to re-run, doing so may overload environment.\n",
      "  --config=file  Customize env vars using a setvars.sh configuration file.\n",
      "  --help         Display this help message and exit.\n",
      "  ...            Additional args are passed to individual env/vars.sh scripts\n",
      "                 and should follow this script's arguments.\n",
      "  \n",
      "  Some POSIX shells do not accept command-line options. In that case, you can pass\n",
      "  command-line options via the SETVARS_ARGS environment variable. For example:\n",
      "  \n",
      "  $ SETVARS_ARGS=\"ia32 --config=config.txt\" ; export SETVARS_ARGS\n",
      "  $ . path/to/setvars.sh\n",
      "  \n",
      "  The SETVARS_ARGS environment variable is cleared on exiting setvars.sh.\n",
      "  \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.35.2\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Requirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.35.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.35.2) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.35.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.35.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.35.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.35.2) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.35.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.35.2) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.35.2) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.35.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.35.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.35.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from requests->transformers==4.35.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.35.2) (2023.7.22)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.1\n",
      "    Uninstalling transformers-4.34.1:\n",
      "      Successfully uninstalled transformers-4.34.1\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "intel-extension-for-transformers 1.2.2 requires transformers==4.34.1, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.35.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.1.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.3.52)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: intel_extension_for_pytorch==2.1.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch==2.1.0) (5.9.0)\n",
      "Requirement already satisfied: numpy in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch==2.1.0) (1.24.3)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_pytorch==2.1.0) (23.1)\n"
     ]
    }
   ],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install transformers==4.35.2\n",
    "!pip install torch==2.1.0\n",
    "!pip install intel_extension_for_pytorch==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d9300-aa72-4cb2-bfd9-ee5c2ca391af",
   "metadata": {},
   "source": [
    "#### Loading Libraries and Packages\n",
    "\n",
    "In this cell, we import the core libraries that will be used throughout the notebook. This setup is crucial as it prepares our Python environment with all the necessary tools for our tasks.\r\n",
    "\r\n",
    "- `from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments`: We import key components from the Hugging Face Transformers library. `AutoModelForSequenceClassification` and `AutoTokenizer` are used for loading the model and tokenizer, respectively. `Trainer` and `TrainingArguments` are essential for setting up and running our model training.\r\n",
    "- `from datasets import load_dataset`: This import from the `datasets` library allows us to easily load and preprocess datasets available in Hugging Face's datasets hub.\r\n",
    "- `import numpy as np`: Numpy is a fundamental package for scientific computing in Python. It provides support for arrays, mathematical operations, and various utility functions.\r\n",
    "- `from sklearn.metrics import accuracy_score`: We import the `accuracy_score` function from Scikit-Learn to calculate the accuracy of our model predictions during evaluation. This metric will help us quantify the performance of our fine-tuned model.\r\n",
    "\r\n",
    "Overall, this cell lays the foundation for our machine learning tasks by equipping us with the necessary libraries and modules.t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc4acc2-5808-449f-a55b-bd3484ad3236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22e9f-4e4e-4672-bdba-b264f5893d41",
   "metadata": {},
   "source": [
    "#### Dataset Loading\n",
    "\n",
    "Here, we load the `emotion` dataset from Hugging Face's datasets library. This dataset will be used for training and evaluating our DistilBERT model, providing a practical context for emotion classification in text.k..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89638882-e384-46a7-93c4-de9b60d33e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc4f80-7806-4bc8-b251-dd3d8e1d5189",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Initialization\r\n",
    "\r\n",
    "In this cell, we initialize the `distilbert-base-uncased` model and its corresponding tokenizer for sequence classification. This setup is the first step in preparing our model for fine-tuning on the emotion classification task..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882dcc84-5752-489f-9e0d-adcd1b8201a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad31f1-2669-4018-9629-ff6ba02af7d3",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\r\n",
    "\r\n",
    "Data preprocessing is essential for model training. We define and apply a preprocessing function that tokenizes our text data, making it compatible with the DistilBERT model's input requirements.\r\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114b861c-54c8-4a43-95dc-94975ac1ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf58cd-55a0-4bec-88e5-930beecf5e78",
   "metadata": {},
   "source": [
    "#### Training with IPEX\r\n",
    "\r\n",
    "This cell is where the integration of Intel Extension for PyTorch (IPEX) comes into play. We define training arguments, including enabling BF16 and IPEX, and set up our Hugging Face trainer. The model is then trained on the emotion dataset, utilizing the enhanced capabilities provided by IPEX..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0798d9-7d90-41c0-ac7b-4d1b0370873d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/transformers/training_args.py:1281: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:507: UserWarning: IPEX CPU does not support fused/fused split update for <class 'torch.optim.adamw.AdamW'> will use non-fused master weight update for bf16 training on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.254651</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:469: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.5034849090576172, metrics={'train_runtime': 405.7114, 'train_samples_per_second': 39.437, 'train_steps_per_second': 2.465, 'total_flos': 2120007057408000.0, 'train_loss': 0.5034849090576172, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    bf16=True, \n",
    "    use_ipex=True,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1046c73-336d-4edb-9210-71a50571583e",
   "metadata": {},
   "source": [
    "#### Model Evaluation\r\n",
    "\r\n",
    "Post-training, we evaluate the model's performance on the validation dataset. This evaluation will give us insights into the effectiveness of our training and the accuracy of the model in emotion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7678f5-385d-44fc-857b-c501712ae8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:469: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2546507716178894,\n",
       " 'eval_accuracy': 0.9195,\n",
       " 'eval_runtime': 7.6411,\n",
       " 'eval_samples_per_second': 261.743,\n",
       " 'eval_steps_per_second': 16.359,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7c017-dcdf-4d79-8441-0eb3eb9ae50f",
   "metadata": {},
   "source": [
    "#### Inference and Testing\r\n",
    "\r\n",
    "Finally, we test the fine-tuned model's inference capabilities on new sentences. This step involves preprocessing the test sentences, performing predictions, and mapping these predictions to human-readable labels. It allows us to visually inspect the model's ability to classify emotions in various text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300ea670-26d0-439c-8b1f-e2156ba8f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I am feeling incredibly happy and joyful today!' - Emotion Prediction: joy\n",
      "Sentence: 'I am so sad and down.' - Emotion Prediction: sadness\n",
      "Sentence: 'I have mixed feelings about this.' - Emotion Prediction: sadness\n",
      "Sentence: 'This is absolutely terrifying!' - Emotion Prediction: fear\n"
     ]
    }
   ],
   "source": [
    "# Define test sentences\n",
    "test_sentences = [\n",
    "    \"I am feeling incredibly happy and joyful today!\",\n",
    "    \"I am so sad and down.\",\n",
    "    \"I have mixed feelings about this.\",\n",
    "    \"This is absolutely terrifying!\",\n",
    "]\n",
    "\n",
    "# Preprocess the test sentences\n",
    "encoded_input = tokenizer(test_sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Predict using the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    predictions = model(**encoded_input)\n",
    "\n",
    "# Convert predictions to human-readable labels\n",
    "predicted_labels = np.argmax(predictions.logits.numpy(), axis=1)\n",
    "\n",
    "# Mapping for the 'emotion' dataset labels\n",
    "label_map = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
    "\n",
    "# Print predictions\n",
    "for sentence, label_id in zip(test_sentences, predicted_labels):\n",
    "    print(f\"Sentence: '{sentence}' - Emotion Prediction: {label_map[label_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46160e25-50d6-48b6-8353-851320c95794",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Throughout this workshop, we have explored the integration of Intel optimizations with Hugging Face's powerful Transformers library. By fine-tuning the DistilBERT model with the support of Intel Extension for PyTorch, we observed enhanced training speeds and efficient utilization of computational resources, especially notable in mixed precision training scenarios.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The exercise showcased not only the technical prowess of combining Hugging Face with Intel optimizations but also highlighted the practical benefits such as reduced training times and resource efficiency. This understanding is pivotal for developers working on NLP tasks, seeking to optimize model performance on Intel hardware. As AI and NLP continue to evolve, harnessing these optimizations will be key in developing more efficient and powerful AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
