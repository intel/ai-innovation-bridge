{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebdcd72-b25b-4f60-9f87-5a7f75df466c",
   "metadata": {},
   "source": [
    "# ITREX - Leveraging Intel Optimizations for Enhanced Inference with Hugging Face\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*AfoKgyTN6l7xNg30GmbsLg.png\" alt=\"Alt Text\" style=\"width: 800px;\"/>\n",
    "\n",
    "Welcome to this developer-focused workshop, where we explore the integration of Intel extensions with Hugging Face models for optimized inference. The goal of this notebook is to demonstrate how developers can use Intel's extensions to achieve efficient and performant inference in production applications.\n",
    "\n",
    "## Why Intel Optimizations Matter\n",
    "\n",
    "In the realm of machine learning, particularly in NLP, the ability to perform efficient and speedy inference is crucial. By using the Intel extension for Transformers, we can load models directly from the Hugging Face Hub, like the \"Intel/neural-chat-7b-v1-1\" model, and optimize them for high-performance inference.\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- **Model Optimization**: Learn how to load and optimize Hugging Face models using Intel's neural compressor and extension APIs.\n",
    "- **Streaming Output**: We'll use the TextStreamer functionality from Hugging Face Transformers to deliver a constant stream of tokens, enhancing the user experience by avoiding large text dumps.\n",
    "- **Intel's Neural Chat Model**: Explore the \"Intel/neural-chat-7b-v1-1\" model, fine-tuned on Gaudi 2 processors, to understand its capabilities in generating text based on input prompts.\n",
    "- **Practical Application**: Understand how these optimizations can be applied in real-world scenarios to deliver performant inference with minimal code.\n",
    "\n",
    "By the end of this notebook, you'll have a practical understanding of how to apply Intel's optimizations to Hugging Face models for efficient inference.\n",
    "\n",
    "Let's dive in and explore the power of optimized model inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install transformers==4.34.1\n",
    "!pip install intel_extension_for_transformers==1.2.2\n",
    "!pip install intel_extension_for_pytorch==2.1.100\n",
    "!pip install tqdm\n",
    "!pip install einops\n",
    "!pip install neural_speed==0.2\n",
    "!pip install torch==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e56fd8-e677-4823-a511-149400b1ab78",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries\n",
    "\n",
    "This cell sets the foundation for our model optimization and text generation tasks. We import:\n",
    "- `AutoTokenizer` and `TextStreamer` from Hugging Face's `transformers` library, crucial for tokenizing our input text and streaming the model's output.\n",
    "- `AutoModelForCausalLM` from `intel_extension_for_transformers`, which is a specialized version of the model class optimized for Intel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc37ad1-91cc-416e-b2a9-d9a95338178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e1cd5-0572-4627-b35a-0245a2acec36",
   "metadata": {},
   "source": [
    "#### Model and Prompt Setup\n",
    "\n",
    "Here, we specify the model and the initial text prompt for our text generation task.\n",
    "- `model_name`: We set this to \"Intel/neural-chat-7b-v1-1\", a model fine-tuned on Intel's hardware, available on the Hugging Face model hub.\n",
    "- `prompt`: This is our starting text for the model to generate from, setting the context for the text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36c711-5c3f-4480-8edc-077afeb8a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Intel/neural-chat-7b-v1-1\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a fisherman at sea,\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90516c0d-2462-4756-b740-7197906d3077",
   "metadata": {},
   "source": [
    "#### Tokenizer Initialization and Input Preparation\n",
    "\n",
    "In this cell, we initialize the tokenizer with our chosen model and prepare our input text for the model.\n",
    "- `tokenizer`: Loaded with the `AutoTokenizer.from_pretrained` method, tailored for our specific model.\n",
    "- `inputs`: The prompt is tokenized to be fed into the model.\n",
    "- `streamer`: An instance of `TextStreamer` is created with our tokenizer, enabling efficient and user-friendly text generation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6a306-745f-4951-8597-e32e55c47d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503933d0-c181-44dd-87d9-0e643ace3145",
   "metadata": {},
   "source": [
    "#### Model Loading and Text Generation\n",
    "\n",
    "This is where the action happens:\n",
    "- We load our model using `AutoModelForCausalLM.from_pretrained`, with `load_in_4bit=True` to enable optimized inference.\n",
    "- The model's `generate` function is called with the `streamer` parameter, which enables streaming output of the text. We set `max_new_tokens` to 300 to control the length of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b58d03-6d3c-4b3b-86b9-9ab29bc70d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5d005-a591-429c-8cac-4137e7b60198",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588dfd27-b61b-4e3a-9445-83277b0695da",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This workshop demonstrated the practical application of Intel optimizations in conjunction with Hugging Face's powerful Transformers library. We explored the nuances of model loading, tokenization, and efficient text generation using Intel's neural compressor and extension APIs.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The skills and knowledge gained here are essential for developers looking to implement optimized NLP models in production environments. The ability to generate text in a streamed manner and leverage Intel's hardware optimizations showcases the potential for building responsive and efficient AI-powered applications.\n",
    "\n",
    "As we continue to advance in the field of AI, understanding and applying such optimizations will be crucial for developers to stay ahead in creating high-performance, scalable, and user-friendly applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
