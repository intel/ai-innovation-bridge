{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebdcd72-b25b-4f60-9f87-5a7f75df466c",
   "metadata": {},
   "source": [
    "# ITREX - Leveraging Intel Optimizations for Enhanced Inference with Hugging Face\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*AfoKgyTN6l7xNg30GmbsLg.png\" alt=\"Alt Text\" style=\"width: 800px;\"/>\n",
    "\n",
    "Welcome to this developer-focused workshop, where we explore the integration of Intel extensions with Hugging Face models for optimized inference. The goal of this notebook is to demonstrate how developers can use Intel's extensions to achieve efficient and performant inference in production applications.\n",
    "\n",
    "## Why Intel Optimizations Matter\n",
    "\n",
    "In the realm of machine learning, particularly in NLP, the ability to perform efficient and speedy inference is crucial. By using the Intel extension for Transformers, we can load models directly from the Hugging Face Hub, like the \"Intel/neural-chat-7b-v1-1\" model, and optimize them for high-performance inference.\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- **Model Optimization**: Learn how to load and optimize Hugging Face models using Intel's neural compressor and extension APIs.\n",
    "- **Streaming Output**: We'll use the TextStreamer functionality from Hugging Face Transformers to deliver a constant stream of tokens, enhancing the user experience by avoiding large text dumps.\n",
    "- **Intel's Neural Chat Model**: Explore the \"Intel/neural-chat-7b-v1-1\" model, fine-tuned on Gaudi 2 processors, to understand its capabilities in generating text based on input prompts.\n",
    "- **Practical Application**: Understand how these optimizations can be applied in real-world scenarios to deliver performant inference with minimal code.\n",
    "\n",
    "By the end of this notebook, you'll have a practical understanding of how to apply Intel's optimizations to Hugging Face models for efficient inference.\n",
    "\n",
    "Let's dive in and explore the power of optimized model inference!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ":: WARNING: setvars.sh has already been run. Skipping re-execution.\n",
      "   To force a re-execution of setvars.sh, use the '--force' option.\n",
      "   Using '--force' can result in excessive use of your environment variables.\n",
      "  \n",
      "usage: source setvars.sh [--force] [--config=file] [--help] [...]\n",
      "  --force        Force setvars.sh to re-run, doing so may overload environment.\n",
      "  --config=file  Customize env vars using a setvars.sh configuration file.\n",
      "  --help         Display this help message and exit.\n",
      "  ...            Additional args are passed to individual env/vars.sh scripts\n",
      "                 and should follow this script's arguments.\n",
      "  \n",
      "  Some POSIX shells do not accept command-line options. In that case, you can pass\n",
      "  command-line options via the SETVARS_ARGS environment variable. For example:\n",
      "  \n",
      "  $ SETVARS_ARGS=\"ia32 --config=config.txt\" ; export SETVARS_ARGS\n",
      "  $ . path/to/setvars.sh\n",
      "  \n",
      "  The SETVARS_ARGS environment variable is cleared on exiting setvars.sh.\n",
      "  \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (4.34.1)\n",
      "Requirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: intel_extension_for_transformers in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_transformers) (23.1)\n",
      "Requirement already satisfied: numpy in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_transformers) (1.24.3)\n",
      "Requirement already satisfied: schema in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from intel_extension_for_transformers) (0.7.5)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from intel_extension_for_transformers) (6.0)\n",
      "Requirement already satisfied: neural-compressor in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from intel_extension_for_transformers) (2.3.2)\n",
      "Requirement already satisfied: transformers==4.34.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from intel_extension_for_transformers) (4.34.1)\n",
      "Requirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.34.1->intel_extension_for_transformers) (4.65.0)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (1.2.14)\n",
      "Requirement already satisfied: opencv-python-headless in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (4.8.1.78)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (2.0.3)\n",
      "Requirement already satisfied: Pillow in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (10.0.0)\n",
      "Requirement already satisfied: prettytable in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (3.9.0)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (9.0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (1.2.2)\n",
      "Requirement already satisfied: pycocotools in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from neural-compressor->intel_extension_for_transformers) (2.0.7)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from schema->intel_extension_for_transformers) (21.6.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from deprecated>=1.2.13->neural-compressor->intel_extension_for_transformers) (1.16.0)\n",
      "Requirement already satisfied: fsspec in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1->intel_extension_for_transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1->intel_extension_for_transformers) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2023.3)\n",
      "Requirement already satisfied: wcwidth in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from prettytable->neural-compressor->intel_extension_for_transformers) (0.2.5)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from pycocotools->neural-compressor->intel_extension_for_transformers) (3.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.34.1->intel_extension_for_transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.34.1->intel_extension_for_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from requests->transformers==4.34.1->intel_extension_for_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.34.1->intel_extension_for_transformers) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->neural-compressor->intel_extension_for_transformers) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (4.65.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: einops in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install transformers\n",
    "!pip install intel_extension_for_transformers\n",
    "!pip install tqdm\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e56fd8-e677-4823-a511-149400b1ab78",
   "metadata": {},
   "source": [
    "#### Importing Required Libraries\n",
    "\n",
    "This cell sets the foundation for our model optimization and text generation tasks. We import:\n",
    "- `AutoTokenizer` and `TextStreamer` from Hugging Face's `transformers` library, crucial for tokenizing our input text and streaming the model's output.\n",
    "- `AutoModelForCausalLM` from `intel_extension_for_transformers`, which is a specialized version of the model class optimized for Intel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc37ad1-91cc-416e-b2a9-d9a95338178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from intel_extension_for_transformers.transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e1cd5-0572-4627-b35a-0245a2acec36",
   "metadata": {},
   "source": [
    "#### Model and Prompt Setup\n",
    "\n",
    "Here, we specify the model and the initial text prompt for our text generation task.\n",
    "- `model_name`: We set this to \"Intel/neural-chat-7b-v1-1\", a model fine-tuned on Intel's hardware, available on the Hugging Face model hub.\n",
    "- `prompt`: This is our starting text for the model to generate from, setting the context for the text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f36c711-5c3f-4480-8edc-077afeb8a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Intel/neural-chat-7b-v1-1\"     # Hugging Face model_id or local model\n",
    "prompt = \"Once upon a time, there existed a fisherman at sea,\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90516c0d-2462-4756-b740-7197906d3077",
   "metadata": {},
   "source": [
    "#### Tokenizer Initialization and Input Preparation\n",
    "\n",
    "In this cell, we initialize the tokenizer with our chosen model and prepare our input text for the model.\n",
    "- `tokenizer`: Loaded with the `AutoTokenizer.from_pretrained` method, tailored for our specific model.\n",
    "- `inputs`: The prompt is tokenized to be fed into the model.\n",
    "- `streamer`: An instance of `TextStreamer` is created with our tokenizer, enabling efficient and user-friendly text generation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f6a306-745f-4951-8597-e32e55c47d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503933d0-c181-44dd-87d9-0e643ace3145",
   "metadata": {},
   "source": [
    "#### Model Loading and Text Generation\n",
    "\n",
    "This is where the action happens:\n",
    "- We load our model using `AutoModelForCausalLM.from_pretrained`, with `load_in_4bit=True` to enable optimized inference.\n",
    "- The model's `generate` function is called with the `streamer` parameter, which enables streaming output of the text. We set `max_new_tokens` to 300 to control the length of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b58d03-6d3c-4b3b-86b9-9ab29bc70d13",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 06:22:11 [INFO] CPU device is used.\n",
      "2024-01-30 06:22:11 [INFO] Applying Weight Only Quantization.\n",
      "2024-01-30 06:22:11 [INFO] Using LLM runtime.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd: ['python', PosixPath('/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages/intel_extension_for_transformers/llm/runtime/graph/scripts/convert_mpt.py'), '--outfile', 'runtime_outs/ne_mpt_f32.bin', '--outtype', 'f32', 'Intel/neural-chat-7b-v1-1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "transformer.wte.weight torch.Size([50279, 4096]) torch.float32\n",
      "transformer.blocks.0.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.0.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.0.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.0.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.0.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.0.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.1.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.1.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.1.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.1.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.1.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.1.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.2.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.2.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.2.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.2.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.2.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.2.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.3.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.3.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.3.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.3.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.3.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.3.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.4.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.4.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.4.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.4.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.4.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.4.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.5.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.5.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.5.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.5.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.5.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.5.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.6.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.6.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.6.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.6.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.6.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.6.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.7.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.7.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.7.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.7.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.7.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.7.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.8.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.8.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.8.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.8.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.8.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.8.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.9.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.9.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.9.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.9.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.9.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.9.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.10.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.10.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.10.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.10.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.10.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.10.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.11.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.11.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.11.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.11.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.11.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.11.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.12.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.12.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.12.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.12.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.12.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.12.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.13.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.13.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.13.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.13.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.13.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.13.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.14.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.14.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.14.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.14.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.14.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.14.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.15.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.15.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.15.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.15.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.15.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.15.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.16.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.16.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.16.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.16.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.16.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.16.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.17.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.17.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.17.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.17.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.17.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.17.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.18.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.18.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.18.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.18.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.18.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.18.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.19.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.19.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.19.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.19.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.19.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.19.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.20.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.20.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.20.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.20.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.20.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.20.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.21.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.21.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.21.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.21.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.21.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.21.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.22.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.22.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.22.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.22.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.22.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.22.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.23.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.23.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.23.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.23.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.23.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.23.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.24.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.24.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.24.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.24.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.24.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.24.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.25.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.25.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.25.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.25.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.25.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.25.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.26.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.26.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.26.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.26.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.26.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.26.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.27.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.27.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.27.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.27.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.27.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.27.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.28.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.28.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.28.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.28.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.28.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.28.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.29.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.29.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.29.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.29.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.29.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.29.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.30.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.30.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.30.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.30.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.30.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.30.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.blocks.31.norm_1.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.31.attn.Wqkv.weight torch.Size([12288, 4096]) torch.float32\n",
      "transformer.blocks.31.attn.out_proj.weight torch.Size([4096, 4096]) torch.float32\n",
      "transformer.blocks.31.norm_2.weight torch.Size([4096]) torch.float32\n",
      "transformer.blocks.31.ffn.up_proj.weight torch.Size([16384, 4096]) torch.float32\n",
      "transformer.blocks.31.ffn.down_proj.weight torch.Size([4096, 16384]) torch.float32\n",
      "transformer.norm_f.weight torch.Size([4096]) torch.float32\n",
      "{'d_model': 4096, 'n_heads': 32, 'n_layers': 32, 'expansion_ratio': 4, 'max_seq_len': 2048, 'vocab_size': 50279, 'resid_pdrop': 0, 'emb_pdrop': 0, 'learned_pos_emb': True, 'attn_config': {'alibi': True, 'alibi_bias_max': 8, 'attn_impl': 'torch', 'attn_pdrop': 0, 'attn_type': 'multihead_attention', 'attn_uses_sequence_id': False, 'clip_qkv': None, 'prefix_lm': False, 'qk_ln': False, 'softmax_scale': None}, 'init_device': 'cpu', 'logit_scale': None, 'no_bias': True, 'verbose': 0, 'embedding_fraction': 1.0, 'norm_type': 'low_precision_layernorm', 'use_cache': False, 'init_config': {'emb_init_std': None, 'emb_init_uniform_lim': None, 'fan_mode': 'fan_in', 'init_div_is_residual': True, 'init_gain': 0, 'init_nonlinearity': 'relu', 'init_std': 0.02, 'name': 'kaiming_normal_', 'verbose': 0}, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': 'bfloat16', 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['MPTForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'Intel/neural-chat-7b-v1-1', 'transformers_version': '4.34.1', 'auto_map': {'AutoConfig': 'Intel/neural-chat-7b-v1-1--configuration_mpt.MPTConfig', 'AutoModelForCausalLM': 'Intel/neural-chat-7b-v1-1--modeling_mpt.MPTForCausalLM'}, 'model_type': 'mpt', 'tokenizer_name': 'EleutherAI/gpt-neox-20b'}\n",
      "Processing variable: transformer.wte.weight with shape:  (50279, 4096)\n",
      "Processing variable: transformer.blocks.0.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.0.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.0.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.0.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.0.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.0.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.1.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.1.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.1.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.1.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.1.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.1.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.2.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.2.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.2.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.2.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.2.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.2.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.3.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.3.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.3.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.3.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.3.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.3.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.4.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.4.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.4.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.4.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.4.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.4.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.5.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.5.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.5.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.5.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.5.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.5.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.6.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.6.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.6.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.6.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.6.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.6.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.7.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.7.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.7.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.7.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.7.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.7.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.8.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.8.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.8.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.8.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.8.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.8.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.9.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.9.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.9.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.9.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.9.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.9.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.10.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.10.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.10.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.10.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.10.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.10.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.11.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.11.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.11.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.11.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.11.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.11.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.12.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.12.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.12.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.12.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.12.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.12.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.13.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.13.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.13.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.13.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.13.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.13.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.14.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.14.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.14.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.14.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.14.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.14.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.15.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.15.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.15.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.15.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.15.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.15.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.16.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.16.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.16.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.16.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.16.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.16.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.17.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.17.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.17.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.17.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.17.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.17.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.18.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.18.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.18.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.18.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.18.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.18.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.19.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.19.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.19.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.19.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.19.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.19.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.20.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.20.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.20.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.20.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.20.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.20.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.21.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.21.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.21.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.21.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.21.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.21.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.22.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.22.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.22.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.22.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.22.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.22.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.23.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.23.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.23.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.23.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.23.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.23.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.24.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.24.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.24.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.24.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.24.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.24.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.25.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.25.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.25.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.25.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.25.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.25.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.26.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.26.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.26.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.26.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.26.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.26.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.27.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.27.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.27.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.27.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.27.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.27.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.28.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.28.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.28.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.28.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.28.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.28.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.29.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.29.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.29.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.29.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.29.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.29.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.30.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.30.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.30.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.30.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.30.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.30.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.blocks.31.norm_1.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.31.attn.Wqkv.weight with shape:  (12288, 4096)\n",
      "Processing variable: transformer.blocks.31.attn.out_proj.weight with shape:  (4096, 4096)\n",
      "Processing variable: transformer.blocks.31.norm_2.weight with shape:  (4096,)\n",
      "Processing variable: transformer.blocks.31.ffn.up_proj.weight with shape:  (16384, 4096)\n",
      "Processing variable: transformer.blocks.31.ffn.down_proj.weight with shape:  (4096, 16384)\n",
      "Processing variable: transformer.norm_f.weight with shape:  (4096,)\n",
      "Done. Output file: runtime_outs/ne_mpt_f32.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5d005-a591-429c-8cac-4137e7b60198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588dfd27-b61b-4e3a-9445-83277b0695da",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This workshop demonstrated the practical application of Intel optimizations in conjunction with Hugging Face's powerful Transformers library. We explored the nuances of model loading, tokenization, and efficient text generation using Intel's neural compressor and extension APIs.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "The skills and knowledge gained here are essential for developers looking to implement optimized NLP models in production environments. The ability to generate text in a streamed manner and leverage Intel's hardware optimizations showcases the potential for building responsive and efficient AI-powered applications.\n",
    "\n",
    "As we continue to advance in the field of AI, understanding and applying such optimizations will be crucial for developers to stay ahead in creating high-performance, scalable, and user-friendly applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
