{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5ca3da-1b58-4c13-9a14-a20348f6c046",
   "metadata": {},
   "source": [
    "# Harnessing Intel Optimizations for Efficient Model Quantization with Hugging Face\n",
    "<img src=\"https://gestaltit.com/wp-content/uploads/2021/12/quantization-pruning-architecture.jpg\" alt=\"Alt Text\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "## Why Learn About Model Quantization\n",
    "\n",
    "In this developer-focused workshop, we delve into the concept of model quantization and its crucial role in enhancing compute efficiency and reducing latency during inference. Quantization is a process that converts a model from using floating-point numbers to integers, which are computationally less expensive to process. This conversion is essential for deploying models on resource-constrained environments where memory footprint and inference speed are criti\n",
    "\n",
    "<img src=\"https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-1b.png\" alt=\"Alt Text\" style=\"width: 800px;\"/>cal.\n",
    "\n",
    "### Understanding the Trade-off\n",
    "\n",
    "While quantization can significantly boost inference speed, it's vital to understand the trade-offs, particularly concerning model accuracy. Sometimes, reducing model size and computational requirements can lead to a decrease in accuracy. This workshop will focus on 'accuracy aware dynamic quantization,' where we aim to balance the trade-offs between speed and accuracy effectively.\n",
    "\n",
    "### Static vs. Dynamic Quantization\n",
    "\n",
    "Before we dive in, let's clarify two types of quantization:\n",
    "- **Static Quantization**: Involves quantizing the weights and activations of the model but requires a calibration step using representative data.\n",
    "- **Dynamic Quantization**: Quantizes the weights but leaves the activations in floating-point. This method is more flexible as it does not require the calibration step, making it suitable for models where input shapes can vary.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll learn how to use the Optimum Intel library to perform dynamic quantization on a Hugging Face model and understand the implications of this process on model performance and efficiency.\n",
    "\n",
    "Let's get started on this journey to make our models faster and more efficient while maintaining their accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f179d-6ef5-46b5-98ee-61fddaa6a549",
   "metadata": {},
   "source": [
    "#### Setting Up the Environment and Initial Imports\n",
    "\n",
    "In this cell, we import essential libraries and tools needed for our quantization task. These include:\n",
    "- `evaluate` and `optimum.intel.INCQuantizer` for evaluating and quantizing our model.\n",
    "- `load_dataset` for loading our evaluation dataset.\n",
    "- `AutoModelForQuestionAnswering` and `AutoTokenizer` for loading our pre-trained model and tokenizer.\n",
    "- `pipeline` for creating a question-answering pipeline.\n",
    "- `neural_compressor.config` components for configuring our quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc5166-eac5-45f6-8f15-56b628c053ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!python -m pip install optimum==1.16.2\n",
    "!pip install --upgrade-strategy eager optimum[neural-compressor]==1.14.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install datasets==2.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d489b48-dc19-4cf3-ad4b-ae2b94177cbf",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Setting Up Quantization Environment\n",
    "\n",
    "This cell is the starting point of our journey into model quantization. Here, we import a set of libraries and modules that are essential for both setting up our model and preparing it for the quantization process.\n",
    "\n",
    "- `import evaluate`: This import brings in the `evaluate` library, which is crucial for assessing the performance of our model. It provides a straightforward way to evaluate various metrics, which is vital for understanding the impact of quantization on model accuracy.\n",
    "\n",
    "- `from optimum.intel import INCQuantizer`: The `INCQuantizer` from the Optimum Intel library is a key component for this workshop. It is specifically designed to handle the quantization process, allowing us to convert our model into a more efficient format suitable for faster inference.\n",
    "\n",
    "- `from datasets import load_dataset`: We use the `datasets` library to load the dataset required for evaluating our model. This step is essential for ensuring that we have the right data to fine-tune and assess our model's performance.\n",
    "\n",
    "- `from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline`: These imports from the Hugging Face Transformers library are critical for loading our pre-trained model and tokenizer. `AutoModelForQuestionAnswering` and `AutoTokenizer` will be used to set up our model for the question-answering task. The `pipeline` function allows us to create a seamless flow for data processing and inference.\n",
    "\n",
    "- `from neural_compressor.config import AccuracyCriterion, TuningCriterion, PostTrainingQuantConfig`: These imports from the Neural Compressor library are used to configure the quantization process. `AccuracyCriterion` and `TuningCriterion` allow us to set parameters that define the acceptable accuracy loss and the tuning process for quantization. `PostTrainingQuantConfig` provides the necessary configuration for post-training quantization, which is the approach we will be using.\n",
    "\n",
    "Each of these imports plays a vital role in preparing our environment for quantizing a model effectively, setting the stage for the subsequent steps in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7e97d-c3ca-4849-aae1-c95bbfc1875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from optimum.intel import INCQuantizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from neural_compressor.config import AccuracyCriterion, TuningCriterion, PostTrainingQuantConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f08be3-299a-4d3e-8aa1-3479665c0bc3",
   "metadata": {},
   "source": [
    "#### Model and Dataset Preparation\n",
    "\n",
    "We initialize our DistilBERT model and tokenizer specifically tuned for the SQuAD dataset. A subset of the validation set from SQuAD is loaded for evaluation purposes. The `evaluate` library is used to set up an evaluator for the question-answering task, and a pipeline is created for processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e08fb-ab78-4522-a40a-14da03ad3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation\").select(range(64))\n",
    "task_evaluator = evaluate.evaluator(\"question-answering\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024c3d6-5b3a-4b03-88c2-0626c9c3082f",
   "metadata": {},
   "source": [
    "#### Evaluation Function Definition\n",
    "\n",
    "Here, we define `eval_fn`, a function that will be used to evaluate the quantized model's performance. This function integrates our model with the question-answering pipeline and returns the F1 score, a measure of the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50d4d2-2b6b-40ef-b100-cb65c0a0deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model):\n",
    "    qa_pipeline.model = model\n",
    "    metrics = task_evaluator.compute(model_or_pipeline=qa_pipeline, data=eval_dataset, metric=\"squad\")\n",
    "    return metrics[\"f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05e4a9-3617-4776-a60d-a8968e891dec",
   "metadata": {},
   "source": [
    "#### Quantization Configuration\n",
    "\n",
    "In this cell, we set up the configuration for dynamic quantization. We specify our tolerable accuracy loss (5%) and the maximum number of tuning trials (10). The `PostTrainingQuantConfig` is set to a dynamic approach, aligning with our focus on dynamic quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a5d36-26b2-4986-818e-750a1f5795c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the accepted accuracy loss to 5%\n",
    "accuracy_criterion = AccuracyCriterion(tolerable_loss=0.05)\n",
    "# Set the maximum number of trials to 10\n",
    "tuning_criterion = TuningCriterion(max_trials=10)\n",
    "quantization_config = PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\", accuracy_criterion=accuracy_criterion, tuning_criterion=tuning_criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e4a11-0d00-4440-9748-6bad6fc47e8a",
   "metadata": {},
   "source": [
    "#### Model Quantization and Saving\n",
    "\n",
    "We initialize the quantizer with our model and the evaluation function. The model is then quantized according to our defined configuration. The quantized model is saved in the specified directory, enabling us to use or deploy it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717499d-7eed-4753-8044-2874ff6a9ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantizer = INCQuantizer.from_pretrained(model, eval_fn=eval_fn)\n",
    "quantizer.quantize(quantization_config=quantization_config, save_directory=\"dynamic_quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab63e8-e951-47b7-817d-69439c31d55a",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "In this workshop, we've successfully navigated the process of dynamically quantizing a model using the Optimum Intel library. We learned the importance of balancing accuracy and computational efficiency and gained hands-on experience in configuring and applying dynamic quantization to a DistilBERT model#.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The skills and knowledge acquired in this session are critical for developers looking to optimize NLP models for production environments, especially where resource constraints are a consideration. Understanding the nuances of model quantization, particularly in the context of dynamic vs. static approaches, empowers developers to make informed decisions about deploying AI models in various scenarios.\n",
    "\n",
    "As we continue to push the boundaries of AI efficiency, the ability to effectively quantize models while maintaining their performance will be an invaluable asset in the toolkit of any AI practitioner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
