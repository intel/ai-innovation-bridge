{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915234aa-2e83-4430-bd4c-06edcdb19bc9",
   "metadata": {},
   "source": [
    "# Uploading and Sharing Models on Hugging Face Hub with Intel Optimizations\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D12AQETAuI3Jb8DHg/article-cover_image-shrink_720_1280/0/1688135186433?e=2147483647&v=beta&t=TE7Ew2JTFECpXUHFLyDtBdRsBgoONGV4roThKcoHdeA\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "Welcome to this part of the workshop where we focus on sharing our work with the wider AI community. Here, we'll learn how to take a model trained using Hugging Face APIs with the Intel Extension for PyTorch and upload it to the Hugging Face Hub. \n",
    "\n",
    "## Why This is Important\n",
    "\n",
    "Sharing models on platforms like Hugging Face Hub not only contributes to the open-source community but also allows for wider testing, evaluation, and improvement of models by others. This process is critical for collaborative development and advancing the field of AI.\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- **Model Upload**: We will go through the steps of uploading our trained model to the Hugging Face Hub.\n",
    "- **Creating a Model Card**: A model card is crucial for documenting our model. It provides information about the model's purpose, architecture, and training data, guiding other users in understanding and using the model effectively.\n",
    "- **Open Sourcing the Model**: By open-sourcing our model, we contribute to the community and enable collective advancements in AI and NLP.\n",
    "- **Evaluation on Hugging Face Hub**: We'll also look at how our model can be evaluated directly on the Hugging Face Hub.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A Hugging Face account and a token with write permissions are necessary to upload models to the Hub.\n",
    "\n",
    "Let's start by setting up our environment and preparing our model for upload to the Hugging Face Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69d348-ba24-4ec5-ac59-3cfe5a3726cd",
   "metadata": {},
   "source": [
    "### Logging in to Hugging Face\n",
    "\n",
    "Before uploading the model, you need to authenticate with Hugging Face. Ensure you have an account and are logged in. The `notebook_login` function provides an easy way to log in for notebook environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aecae5-f611-4f66-9812-b231cefc1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install  ipywidgets --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e681293-f45d-4ca6-8a9f-7d6f1db43aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force restart kernel to pull latest environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a3f02-32da-46b5-aebb-f2fb5e81e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, Repository\n",
    "\n",
    "# Login to Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10831457-cded-48fb-a3a7-e6f4a842b900",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Loading\n",
    "\n",
    "In this cell, we load our trained model and tokenizer:\n",
    "- We use `AutoModelForSequenceClassification` and `AutoTokenizer` to load the model and tokenizer. The model is loaded from a saved checkpoint, while the tokenizer is loaded using the base DistilBERT tokenizer.\n",
    "- `checkpoint_path` should be set to the path where your model checkpoint is saved. Always target the last checkpoint if the model was trained succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7afd2-3e03-4e74-8674-b5e0426680b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define the path to the checkpoint\n",
    "checkpoint_path = r\"./results/checkpoint-1000\"  # Replace with your checkpoint folder\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd0d4d-03e3-432e-9e65-ddcc6c8637ac",
   "metadata": {},
   "source": [
    "#### Saving and Uploading the Model and Tokenizer\n",
    "\n",
    "Here, we prepare and upload the model and tokenizer to the Hugging Face Hub:\n",
    "- The model and tokenizer are saved locally with the names specified in `model_name_on_hub`.\n",
    "- `push_to_hub` methods are used to upload both the model and tokenizer to the Hugging Face Hub under your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c59a28-8158-42c6-97f1-2723919d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_name_on_hub = \"huggingface-workshop-emotions-bert\"\n",
    "model.save_pretrained(model_name_on_hub)\n",
    "tokenizer.save_pretrained(model_name_on_hub)\n",
    "\n",
    "# Push to the hub\n",
    "model.push_to_hub(model_name_on_hub)\n",
    "tokenizer.push_to_hub(model_name_on_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b157a-2b48-44a5-8fd4-0a8de8f812b9",
   "metadata": {},
   "source": [
    "#### Model Uploaded to Hugging Face Model Hub\n",
    "\n",
    "Congratulations! Your fine-tuned model is now uploaded to the Hugging Face Model Hub. You can view and share your model using its URL: `https://huggingface.co/your-username/your-model-name`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cace9d1-1c22-4e8b-9210-08899c121ca3",
   "metadata": {},
   "source": [
    "## Creating and Uploading the Model Card\n",
    "\n",
    "A model card is a critical document that provides information about the model's purpose, creation, and usage. It enhances transparency and helps users understand and use the model appropriately. Here is a template below"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3160b4b-f04c-4eaf-8801-d9fbd6f0627f",
   "metadata": {},
   "source": [
    "---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- en\n",
    "---\n",
    "# Model Card for Simple DistilBERT Emotions Workshop Model\n",
    "\n",
    "## Model Description\n",
    "- **Purpose**: This model is fine-tuned to perform multi-class emotion classification. It can identify various emotions in text, such as joy, sadness, love, anger, fear, and surprise.\n",
    "- **Model architecture**: The model is based on the `distilbert-base-uncased` architecture, a distilled version of the BERT model which is smaller and faster but retains most of its predictive power.\n",
    "- **Training data**: The model was trained on the `emotion` dataset from Hugging Face's datasets library. This dataset includes text labeled with different emotions. During preprocessing, texts were tokenized, and padding and truncation were applied to standardize their lengths.\n",
    "\n",
    "## Intended Use\n",
    "- **Intended users**: This model is intended for developers and researchers interested in emotion analysis in text, including applications in social media sentiment analysis, customer feedback interpretation, and mental health assessment.\n",
    "- **Use cases**: Potential use cases include analyzing social media posts for emotional content, enhancing chatbots to understand user emotions, and helping mental health professionals in identifying emotional states from text-based communications.\n",
    "\n",
    "## Limitations\n",
    "- **Known limitations**: The model's accuracy may vary depending on the context and the dataset's representativeness. It may not perform equally well on texts from domains significantly different from the training data.\n",
    "\n",
    "## Hardware \n",
    "- **Training Platform**: The model was trained on 4th Generation Intel Xeon Processors available on the Intel Developer Cloud (cloud.intel.com). The training completed in under 8 minutes, demonstrating the efficiency of Intel hardware optimizations.\n",
    "\n",
    "## Ethical Considerations\n",
    "- **Ethical concerns**: Care should be taken to ensure that the model is not used in sensitive applications without proper ethical considerations, especially in scenarios that could impact individual privacy or mental health.\n",
    "\n",
    "## More Information\n",
    "- **Training Setup**: The training leveraged Intel extensions for PyTorch (IPEX) to optimize training efficiency on Intel hardware. Mixed precision training (FP32 and BF16) was enabled, contributing to the rapid training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf9282-e2f0-4331-a558-7637871c5109",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this section of the workshop, we successfully uploaded a model to the Hugging Face Hub. This process included logging into Hugging Face, loading the model and tokenizer, saving them with appropriate names, and finally pushing them to the Hub. #\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The ability to share models via platforms like Hugging Face Hub is invaluable in the field of AI and ML. It not only fosters collaboration and open-source contributions but also provides a platform for model evaluation and improvement by the community. Uploading models with detailed documentation and model cards ensures transparency and usability, paving the way for future advancements and applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
