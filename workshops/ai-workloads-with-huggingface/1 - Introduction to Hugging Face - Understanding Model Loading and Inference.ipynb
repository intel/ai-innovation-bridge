{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee762827-64ac-4764-b228-06af4ecb98cb",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face: Understanding Model Loading and Inference\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "Welcome to our workshop on the fundamentals of Hugging Face, a pivotal platform in the world of Natural Language Processing (NLP) and Machine Learning (ML). This session is designed to guide you through the basic mechanics of Hugging Face, particularly focusing on loading models and performing inference. We will explore both direct methods using the `generate` function from loaded models and the streamlined approach offered by Hugging Face pipelines.\n",
    "\n",
    "## What is Hugging Face?\n",
    "\n",
    "Hugging Face is a revolutionary platform in the AI community, best known for its Transformers library. It provides an extensive collection of pre-trained models that are crucial for a variety of NLP tasks such as text classification, question answering, text generation, and more. These models, built on the transformer architecture, have transformed the way we approach language understanding and generation in AI.\n",
    "\n",
    "## The Value of Hugging Face\n",
    "\n",
    "The true value of Hugging Face lies in its accessibility and ease of use. With just a few lines of code, developers can leverage complex, state-of-the-art models for their NLP needs. This ease of access democratizes cutting-edge AI technology, making it available to a broader range of users and developers. Whether it's fine-tuning models on specific datasets or quickly deploying pre-trained models for inference, Hugging Face simplifies these processes remarkably.\n",
    "\n",
    "## Why Learn Hugging Face?\n",
    "\n",
    "For developers venturing into NLP and ML, Hugging Face is an indispensable tool. It not only accelerates the development process but also provides a platform for experimentation and learning. Understanding how to effectively load models and perform inference is crucial for building robust AI applications. As the field of NLP continues to evolve, proficiency in Hugging Face will become increasingly valuable.\n",
    "\n",
    "## Foundation for the Workshop\n",
    "\n",
    "The skills and concepts we develop here form the foundation for more advanced topics in our workshop. By mastering model loading and inference, you'll be well-prepared to tackle more complex tasks, experiment with different models, and appreciate the nuances of model performance and adaptation. Let's embark on this journey to unlock the full potential of Hugging Face in AI development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af370f22-3335-4ad7-a049-08445f79e37b",
   "metadata": {},
   "source": [
    "#### Environment Setup and Dependency Installation\n",
    "\n",
    "This cell is crucial for setting up the environment and installing necessary dependencies. We install specific versions of the `transformers` and `torch` libraries. These libraries are essential for leveraging the full capabilities of Intel hardware and optimizing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers==4.44.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install torch==2.1.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install tiktoken==0.5.2 --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f36f4e-9605-4e99-9154-f2e76488ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force restart kernel to pull latest environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590e167-5e1f-4a03-85a7-88e86bae7c40",
   "metadata": {},
   "source": [
    "#### Model Loading\n",
    "Here, we load the model and tokenizer from Hugging Face. We're using `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library to load \"microsoft/Phi-3.5-mini-instruct\", a causal language model. The `trust_remote_code` flag is set to `True` for remote code execution, and `torch_dtype` is set to \"auto\" for automatic precision setting, optimizing performance on the underlying hardware..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d006f-248d-4404-bbca-b7910c869406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "\n",
    "model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model,\n",
    "  trust_remote_code=True,\n",
    "  torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46f252-775c-4b12-a454-321ac8f801b0",
   "metadata": {},
   "source": [
    "#### Generating Text\n",
    "In this cell, we perform text generation. We first tokenize a prompt (\"I love learning to code...\") and then use the `generate` method of our model to create new text. The generation parameters like `max_new_tokens`, `temperature`, and `top_p` are configured to control the creativity and coherence of the generated text. The result is then decoded and printed, showing the model's response to our prompt.\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a3e1d-5256-4c42-967f-eb28d8460396",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"I love learning to code...\", return_tensors=\"pt\").to(model.device)\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=64,\n",
    "  temperature=0.70,\n",
    "  top_p=0.95,\n",
    "  do_sample=True,\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128e45f-b309-41b2-9ebe-90bb699e1eff",
   "metadata": {},
   "source": [
    "#### Setting Up a Pipeline for Text Generation\n",
    "This cell demonstrates setting up a Hugging Face pipeline for text generation. We initialize a `pipeline` object with our model and tokenizer, configuring it specifically for text generation. Parameters like `torch_dtype` for precision and `device_map` for device allocation are set for optimal performance. We then use this pipeline to generate a sequence from a given prompt, showcasing an alternative, more streamlined approach to model inference..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054b5af-29c9-471f-acf2-7df0681fa315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23be6-1f97-4a51-8f0d-c38b48200539",
   "metadata": {},
   "source": [
    "#### Displaying Pipeline Results\n",
    "In the final cell, we iterate through the sequences generated by our pipeline and print them out. This step is crucial for visualizing the outputs of our text generation pipeline, allowing us to evaluate the model's performance in generating coherent and contextually relevant text..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13188b2-4585-4524-8b4d-14450fef8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = generator( \n",
    "            \"I love learning to code...\",\n",
    "            max_length=64,\n",
    "            do_sample=True,\n",
    "            top_k=3,\n",
    "            temperature=0.70,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973235f-52bf-47d9-8867-648ba80064d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences:\n",
    "         print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dbe7c-67b4-42bf-a48e-11bec5a8898b",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Throughout this workshop, we've explored the basic mechanics of using Hugging Face, focusing on loading models, performing inference, and utilizing pipelines for efficient text generation. We've leveraged the power of Intel hardware optimizations and the versatility of Hugging Face's Transformers library to achieve high-performance model inference.\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "The exercises demonstrated not only the ease of using pre-trained models from Hugging Face but also the importance of understanding the underlying hardware optimizations. Developers should now have a foundational understanding of model loading, text generation, and the utilization of pipelines in Hugging Face, which are crucial for developing efficient NLP applications. This knowledge serves as a stepping stone to more advanced topics in AI and ML, enabling developers to harness the full potential of modern NLP technologies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
