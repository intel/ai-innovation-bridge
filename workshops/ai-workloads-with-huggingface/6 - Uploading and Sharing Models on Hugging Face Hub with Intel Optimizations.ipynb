{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915234aa-2e83-4430-bd4c-06edcdb19bc9",
   "metadata": {},
   "source": [
    "# Uploading and Sharing Models on Hugging Face Hub with Intel Optimizations\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D12AQETAuI3Jb8DHg/article-cover_image-shrink_720_1280/0/1688135186433?e=2147483647&v=beta&t=TE7Ew2JTFECpXUHFLyDtBdRsBgoONGV4roThKcoHdeA\" alt=\"Alt Text\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "Welcome to this part of the workshop where we focus on sharing our work with the wider AI community. Here, we'll learn how to take a model trained using Hugging Face APIs with the Intel Extension for PyTorch and upload it to the Hugging Face Hub. \n",
    "\n",
    "## Why This is Important\n",
    "\n",
    "Sharing models on platforms like Hugging Face Hub not only contributes to the open-source community but also allows for wider testing, evaluation, and improvement of models by others. This process is critical for collaborative development and advancing the field of AI.\n",
    "\n",
    "### Key Learning Points\n",
    "\n",
    "- **Model Upload**: We will go through the steps of uploading our trained model to the Hugging Face Hub.\n",
    "- **Creating a Model Card**: A model card is crucial for documenting our model. It provides information about the model's purpose, architecture, and training data, guiding other users in understanding and using the model effectively.\n",
    "- **Open Sourcing the Model**: By open-sourcing our model, we contribute to the community and enable collective advancements in AI and NLP.\n",
    "- **Evaluation on Hugging Face Hub**: We'll also look at how our model can be evaluated directly on the Hugging Face Hub.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A Hugging Face account and a token with write permissions are necessary to upload models to the Hub.\n",
    "\n",
    "Let's start by setting up our environment and preparing our model for upload to the Hugging Face Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69d348-ba24-4ec5-ac59-3cfe5a3726cd",
   "metadata": {},
   "source": [
    "### Logging in to Hugging Face\n",
    "\n",
    "Before uploading the model, you need to authenticate with Hugging Face. Ensure you have an account and are logged in. The `notebook_login` function provides an easy way to log in for notebook environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a3f02-32da-46b5-aebb-f2fb5e81e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, Repository\n",
    "\n",
    "# Login to Hugging Face\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10831457-cded-48fb-a3a7-e6f4a842b900",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer Loading\n",
    "\n",
    "In this cell, we load our trained model and tokenizer:\n",
    "- We use `AutoModelForSequenceClassification` and `AutoTokenizer` to load the model and tokenizer. The model is loaded from a saved checkpoint, while the tokenizer is loaded using the base DistilBERT tokenizer.\n",
    "- `checkpoint_path` should be set to the path where your model checkpoint is saved. Always target the last checkpoint if the model was trained succesfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d7afd2-3e03-4e74-8674-b5e0426680b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Define the path to the checkpoint\n",
    "checkpoint_path = r\"./results/checkpoint-2000\"  # Replace with your checkpoint folder\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd0d4d-03e3-432e-9e65-ddcc6c8637ac",
   "metadata": {},
   "source": [
    "#### Saving and Uploading the Model and Tokenizer\n",
    "\n",
    "Here, we prepare and upload the model and tokenizer to the Hugging Face Hub:\n",
    "- The model and tokenizer are saved locally with the names specified in `model_name_on_hub`.\n",
    "- `push_to_hub` methods are used to upload both the model and tokenizer to the Hugging Face Hub under your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c59a28-8158-42c6-97f1-2723919d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_name_on_hub = \"desired-model-name\"\n",
    "model.save_pretrained(model_name_on_hub)\n",
    "tokenizer.save_pretrained(model_name_on_hub)\n",
    "\n",
    "# Push to the hub\n",
    "model.push_to_hub(model_name_on_hub)\n",
    "tokenizer.push_to_hub(model_name_on_hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b157a-2b48-44a5-8fd4-0a8de8f812b9",
   "metadata": {},
   "source": [
    "#### Model Uploaded to Hugging Face Model Hub\n",
    "\n",
    "Congratulations! Your fine-tuned model is now uploaded to the Hugging Face Model Hub. You can view and share your model using its URL: `https://huggingface.co/your-username/your-model-name`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cace9d1-1c22-4e8b-9210-08899c121ca3",
   "metadata": {},
   "source": [
    "## Creating and Uploading the Model Card\n",
    "\n",
    "A model card is a critical document that provides information about the model's purpose, creation, and usage. It enhances transparency and helps users understand and use the model appropriately. Here is a template below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660c612-5fea-43b9-b34c-6126e95db32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_content = \"\"\"\n",
    "# Model Card for My Fine-Tuned Model\n",
    "\n",
    "## Model Description\n",
    "- **Purpose**: [Describe the purpose of your model. What task does it perform?]\n",
    "- **Model architecture**: [Specify the architecture, e.g., BERT, GPT-2, etc.]\n",
    "- **Training data**: [Briefly describe the dataset used for training. Include any data cleaning or preprocessing steps.]\n",
    "\n",
    "## Intended Use\n",
    "- **Intended users**: [Who are the intended users of the model?]\n",
    "- **Use cases**: [Describe potential use cases for the model.]\n",
    "\n",
    "## Limitations\n",
    "- **Known limitations**: [Mention any known limitations of the model.]\n",
    "\n",
    "## Hardware \n",
    "- **Training Platform**: [Describe details about the systems and platform used to train the model.]\n",
    "\n",
    "## Software Optimizations\n",
    "- **Known Optimizations**: [Describe details about any optimizations used during training.]\n",
    "\n",
    "## Ethical Considerations\n",
    "- **Ethical concerns**: [Discuss any ethical concerns related to the use of your model.]\n",
    "\n",
    "## More Information\n",
    "- [Include any additional information, links, or references.]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9fde36-7c64-47e4-9a9f-02a1879e17a1",
   "metadata": {},
   "source": [
    "If you have git-lfs installed, you can try uploading directly. Git-LFS is not available in the IDC free notebook environments at the time of creating this tutorial (1/30/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65be83-978f-448f-b0de-3c0fcb1109e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the model card content to a file\n",
    "model_card_filename = f\"{model_name_on_hub}/README.md\"\n",
    "with open(model_card_filename, \"w\") as file:\n",
    "    file.write(model_card_content)\n",
    "\n",
    "# Push the model card to the hub\n",
    "repo = Repository(model_name_on_hub, clone_from=model_name_on_hub)\n",
    "repo.push_to_hub(commit_message=\"Add model card\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf9282-e2f0-4331-a558-7637871c5109",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this section of the workshop, we successfully uploaded a model to the Hugging Face Hub. This process included logging into Hugging Face, loading the model and tokenizer, saving them with appropriate names, and finally pushing them to the Hub. #\n",
    "\n",
    "## Discussion\n",
    "\n",
    "The ability to share models via platforms like Hugging Face Hub is invaluable in the field of AI and ML. It not only fosters collaboration and open-source contributions but also provides a platform for model evaluation and improvement by the community. Uploading models with detailed documentation and model cards ensures transparency and usability, paving the way for future advancements and applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
