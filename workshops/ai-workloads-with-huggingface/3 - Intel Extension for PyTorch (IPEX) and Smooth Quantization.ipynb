{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7391e29b-055f-40bf-8d2f-a99d51c87ded",
   "metadata": {},
   "source": [
    "# Introduction to Optimizing and Using Pretrained Language Models with Intel Extension for PyTorch (IPEX) and Smooth Quantization\n",
    "\n",
    "In this notebook, we explore the process of loading a pre-trained language model, optimizing it using Intel Extension for PyTorch (IPEX), and generating text responses in a conversational AI context. This notebook demonstrates the practical use of model quantization and optimization techniques to improve performance on Intel hardware.\n",
    "\n",
    "![smoothquant](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*RH6ou7jL5Fw9KwGG.png)\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand how to load and use pretrained language models with Hugging Face's `transformers` library.\n",
    "2. Learn how to apply model optimization and quantization techniques using Intel Extension for PyTorch (IPEX).\n",
    "3. Explore how to tokenize and stream inputs for text generation tasks.\n",
    "4. Execute inference efficiently using a quantized, optimized model.\n",
    "\n",
    "### Technology Summary:\n",
    "- **Hugging Face Transformers:** Provides access to state-of-the-art pretrained models for various NLP tasks, such as text generation, classification, and more.\n",
    "- **Intel Extension for PyTorch (IPEX):** A library that optimizes deep learning models for Intel hardware by applying quantization, optimizations, and inference improvements.\n",
    "- **Quantization:** A technique to reduce model size and improve inference performance by using lower-precision data types for weights.\n",
    "- **Text Generation:** Using a causal language model for generating text responses based on prompts provided by the user.\n",
    "\n",
    "Through this notebook, you'll see how these technologies work together to build an efficient and powerful AI system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05270b68-8c14-4bda-9e23-8a2fe356045c",
   "metadata": {},
   "source": [
    "## Environment Setup for the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff05e4-72e8-44bb-86fd-f972b021fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install intel-extension-for-pytorch==2.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install transformers==4.35.2 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install torch==2.2.0 --no-warn-script-location > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3d203-ecda-49ff-90c8-67405204573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force restart kernel to pull latest environment\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f2552-8a4d-4d4a-9290-ea09da11bcee",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries\n",
    "\n",
    "In this cell, we import essential libraries for model loading and optimization:\n",
    "\n",
    "- **torch:** The PyTorch framework used for deep learning.\n",
    "- **intel_extension_for_pytorch (ipex):** Intelâ€™s extension to PyTorch for optimizing deep learning models on Intel hardware.\n",
    "- **transformers:** A library from Hugging Face that provides access to pretrained models and tokenizers.\n",
    "    - **AutoTokenizer:** Automatically loads the appropriate tokenizer for the model.\n",
    "    - **AutoModelForCausalLM:** Loads a pre-trained causal language model for tasks like text generation.\n",
    "    - **TextStreamer:** A utility for streaming and handling text generation from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66175c3b-e208-4e4c-8981-06539f962dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e767a1-9f93-427c-89f3-7ed9b22fe76f",
   "metadata": {},
   "source": [
    "## Load Pretrained Model and Tokenizer\n",
    "\n",
    "In this cell, we load the pre-trained language model and tokenizer from Hugging Face's `Intel/neural-chat-7b-v3-3` model. \n",
    "\n",
    "### Neural-Chat-v3-3 Model Overview\n",
    "\n",
    "This model is a fine-tuned 7B parameter LLM on the Intel Gaudi 2 processor from the Intel/neural-chat-7b-v3-1 on the meta-math/MetaMathQA dataset. The model was aligned using the Direct Performance Optimization (DPO) method with Intel/orca_dpo_pairs. The Intel/neural-chat-7b-v3-1 was originally fine-tuned from mistralai/Mistral-7B-v-0.1. For more information, refer to the blog The Practice of Supervised Fine-tuning and Direct Preference Optimization on Intel Gaudi2.\n",
    "\n",
    "- **Base Model:** `mistralai/Mistral-7B-v-0.1`\n",
    "- **Context Length:** 8192 tokens\n",
    "- **License:** Apache 2.0\n",
    "\n",
    "You can use this model for various language-related tasks like math problem solving and generating coherent text. Check the [LLM Leaderboard](https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3-3) for evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96bf6e-74a0-48e7-850b-65a8ef9886a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = 'Intel/neural-chat-7b-v3-3'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(Model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26f4e7-7492-4db9-a40e-68a899b84211",
   "metadata": {},
   "source": [
    "## Model Optimization and Quantization with Intel Extension for PyTorch (IPEX)\n",
    "\n",
    "This cell performs model optimization using Intel's IPEX library, including applying quantization to improve inference speed and reduce memory usage.\n",
    "\n",
    "- **qconfig:** Configuration for weight-only quantization.\n",
    "  - **weight_dtype:** Specifies the data type for weights, here using `torch.quint4x2` for quantization, with the option for `torch.qint8`.\n",
    "  - **lowp_mode:** Specifies the mode for lower precision. Options include `NONE`, `FP16`, `BF16`, and `INT8`.\n",
    "- **checkpoint:** Optional parameter to load a pre-quantized checkpoint (e.g., INT4 or INT8).\n",
    "\n",
    "- **model_ipex:** Optimizes the loaded model using IPEX, applying the specified quantization configuration.\n",
    "  - **ipex.llm.optimize:** Optimizes the model for better performance on Intel hardware.\n",
    "\n",
    "After optimization, the original `model` object is deleted to free memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1a21d-9126-41df-a525-59a349a43ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(\n",
    "  weight_dtype=torch.quint4x2, # or torch.qint8\n",
    "  lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8\n",
    ")\n",
    "checkpoint = None # optionally load int4 or int8 checkpoint\n",
    "\n",
    "# PART 3: Model optimization and quantization\n",
    "model_ipex = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint)\n",
    "\n",
    "del model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06446ca9-5c35-49d0-bcaa-912081b2fd65",
   "metadata": {},
   "source": [
    "## Preparing System Message and User Prompt for Model Input\n",
    "\n",
    "In this cell, we define the system message and user prompt to guide the model's response, followed by tokenizing the input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6c920-5e09-43e2-80a2-fe13e07cc98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message= \"\"\"\\n\\n You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "prompt= \"\\n\\n You are an expert in astronomy. Can you tell me 5 fun facts about the universe?\"\n",
    "model_answer_1 = 'None'\n",
    "\n",
    "prompt_tempate = f\"\"\"\n",
    "### System:\n",
    "{system_message}\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt_tempate, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959f01c-c826-4ecd-9a47-85ab35ee69f7",
   "metadata": {},
   "source": [
    "## Generating Model Response with Streamer\n",
    "\n",
    "In this cell, we generate a response from the optimized model using the provided input tokens and stream the output.\n",
    "\n",
    "- **streamer:** Utilizes `TextStreamer` to handle the streaming of generated tokens.\n",
    "  - **skip_prompt:** Ensures that the initial prompt is not repeated in the streamed output.\n",
    "\n",
    "- **torch.inference_mode():** A context manager that disables gradient computation, optimizing the model for inference.\n",
    "\n",
    "- **model_ipex.generate:** Generates a sequence of tokens based on the input provided.\n",
    "  - **inputs:** The tokenized input prompt for the model.\n",
    "  - **streamer:** Streams the output using the `TextStreamer`.\n",
    "  - **max_new_tokens:** Limits the number of new tokens generated to 300.\n",
    "  - **repetition_penalty:** Penalizes repeated phrases or words by setting a repetition penalty, encouraging more diverse outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658fb922-8b01-4f09-9bd1-28e949aefc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(tokenizer,skip_prompt=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    tokens = model_ipex.generate(\n",
    "        inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=300,\n",
    "        repetition_penalty=1.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff30eb7-e261-41a9-8b3d-15b2c7ff1cc1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we successfully demonstrated how to load, optimize, and generate text responses using a pretrained language model with Intel's IPEX library. By applying quantization techniques, we were able to enhance the model's performance on Intel hardware, while maintaining the quality of the generated text. This workflow can be extended to a variety of use cases, from conversational AI to content generation, enabling efficient and scalable deployments in real-world applications.\n",
    "\n",
    "### Key Takeaways:\n",
    "- Pretrained models from Hugging Face can be optimized for performance using Intel's IPEX.\n",
    "- Quantization is an effective technique for improving the speed and memory efficiency of models.\n",
    "- Streaming text generation enables real-time responses in applications like chatbots or virtual assistants.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
