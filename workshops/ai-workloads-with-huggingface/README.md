# Enabling AI Workloads and Optimizations on Intel Hardware with Hugging Face

Welcome to the repository for the workshop "Enabling AI Workloads and Optimizations on Intel Hardware with Hugging Face," originally delivered on January 3, 2024. This workshop provides comprehensive insights and practical examples of leveraging various Intel optimizations in conjunction with Hugging Face's powerful tools and libraries for AI development.

Updated on : 8/22/24

## Repository Structure

This repository contains a total of 6 notebooks, each designed to guide you through different aspects of utilizing Intel optimizations with Hugging Face technologies:

#### 1 - Introduction to Hugging Face - Understanding Model Loading and Inference
An easy introduction to accessing and inferring models using Hugging Face tools.

#### 2 - Leveraging Intel Optimizations with Hugging Face for Enhanced Model Performance
Demonstrates how to improve training performance by utilizing Intel Extension for PyTorch.

#### 3 - Intel Extension for PyTorch (IPEX) and Smooth Quantization
Guides on using Intel Extension for Pytorch and Smooth Quant to Optimize Inference

#### 4 - Enhancing Inference Performance with Optimum Intel and OpenVINO
Explains how to utilize Optimum Intel alongside the OpenVINO toolkit for optimized model inference.

#### 5 - Uploading and Sharing Models on Hugging Face Hub with Intel Optimizations
Demonstrates the process of sharing a trained model with optimizations on the Hugging Face Hub, contributing to the open-source community.
